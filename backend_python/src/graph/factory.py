import logging
from typing import List
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langchain_core.messages import SystemMessage, ToolMessage
from langchain_core.language_models import BaseChatModel
from langchain_core.tools import BaseTool

from src.graph.state import AgentState
from src.graph.context_builder import ContextBuilder
from src.services.intent_classifier import IntentClassifier
from src.graph.edges import route_step
from src.agents.sop_manager import SOPManager
from src.models.reasoning import ReasoningStep

logger = logging.getLogger(__name__)

class AgentGraphFactory:
    """
    Factory for creating the LangGraph Agent.
    Decouples graph definition from global state and specific LLM instances.
    """

    def __init__(self, llm: BaseChatModel, reasoning_llm: BaseChatModel, tools: List[BaseTool]):
        self.llm = llm
        self.reasoning_llm = reasoning_llm
        self.tools = tools
        # Bind tools once for the execution model
        self.llm_with_tools = self.llm.bind_tools(tools)

    def create_graph(self):
        """Builds and compiles the StateGraph."""
        
        # 1. Define Nodes
        
        def reasoning_node(state: AgentState):
            """Tier 1: High-level planning."""
            messages = state["messages"]
            
            # Context Injection
            active_prompt = ContextBuilder.build_system_prompt(state)
            
            # Filter system messages & Prepend new one
            cleaned_messages = [msg for msg in messages if not isinstance(msg, SystemMessage)]
            reasoning_messages = [SystemMessage(content=active_prompt)] + cleaned_messages
            
            # Logic Model
            reasoning_model = self.reasoning_llm.with_structured_output(ReasoningStep)
            
            try:
                logger.info("ðŸ¤” Reasoning Node: Thinking...")
                step = reasoning_model.invoke(reasoning_messages)
                
                logger.info(f"ðŸ’¡ Thought: {step.analysis}")
                logger.info(f"ðŸ‘‰ Action: {step.action} (Tool: {step.tool_name})")
                
                return {
                    "internal_plan": [step.model_dump()],
                    "thought_log": [step.analysis]
                }
            except Exception as e:
                logger.error(f"âŒ Reasoning Failed: {e}")
                return {
                    "internal_plan": [{
                        "analysis": "Internal Reasoning Error",
                        "action": "terminate",
                        "validation_passed": False
                    }]
                }

        def execution_node(state: AgentState):
            """Tier 3: Execution and Tool Calling."""
            messages = state["messages"]
            internal_plan = state.get("internal_plan", [])
            latest_plan = internal_plan[-1] if internal_plan else None
            
            # Context Injection
            active_prompt = ContextBuilder.build_system_prompt(state)
            
            # Filter & Prepend
            cleaned_messages = [msg for msg in messages if not isinstance(msg, SystemMessage)]
            exec_messages = [SystemMessage(content=active_prompt)] + cleaned_messages
            
            # Deterministic routing based on plan
            tool_to_call = None
            if latest_plan and latest_plan.get("action") == "call_tool":
                tool_to_call = latest_plan.get("tool_name")
            
            if tool_to_call:
                # ðŸ›¡ï¸ Loop Guard Check (Simplified for Factory)
                last_msg = messages[-1] if messages else None
                if isinstance(last_msg, ToolMessage) and last_msg.name == tool_to_call:
                     # Check redundancy logic (omitted for brevity, can re-import if complex)
                     pass

                # RBTA Check
                available_tools = SOPManager.get_available_tools(state)
                target_tool = next((t for t in available_tools if t.name == tool_to_call), None)
                
                if target_tool:
                     response = self.llm.bind_tools([target_tool], tool_choice=tool_to_call).invoke(exec_messages)
                else:
                     logger.warning(f"ðŸ›‘ Security Block: Tool '{tool_to_call}' not allowed.")
                     response = self.llm.invoke(exec_messages + [SystemMessage(content=f"SYSTEM ALERT: Tool {tool_to_call} disallowed.")])
            else:
                # Pure conversation
                available_tools = SOPManager.get_available_tools(state)
                response = self.llm.bind_tools(available_tools).invoke(exec_messages)
                
            return {"messages": [response], "phase": "EXECUTION"}

        async def gatekeeper(state: AgentState) -> str:
            """Optimized Entry Point."""
            return await IntentClassifier.classify_intent(state["messages"])

        def should_continue(state: AgentState) -> str:
            """Check if we should go to tools."""
            last_message = state["messages"][-1]
            if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
                return "tools"
            return END

        # 2. Build Graph
        workflow = StateGraph(AgentState)
        
        workflow.add_node("reasoning", reasoning_node)
        workflow.add_node("execution", execution_node)
        workflow.add_node("tools", ToolNode(self.tools))
        
        # Edges
        # Note: set_conditional_entry_point supports async functions
        workflow.set_conditional_entry_point(
            gatekeeper,
            {"reasoning": "reasoning", "execution": "execution"}
        )
        
        workflow.add_conditional_edges(
            "reasoning",
            route_step,
            {"execution": "execution", "tools": "tools", END: END}
        )
        
        workflow.add_conditional_edges(
            "execution",
            should_continue,
            {"tools": "tools", END: END}
        )
        
        workflow.add_edge("tools", "reasoning")
        
        return workflow.compile()
